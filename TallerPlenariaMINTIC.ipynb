{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TallerPlenariaMINTIC.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOfMtUg045AIQBYgK7nGJIy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/elizleon/PLN/blob/main/TallerPlenariaMINTIC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-jmSH4hqKlT"
      },
      "source": [
        "<center><h1>Practica Análisis de Texto</h1>\n",
        "<center><h1>Plenaria MINTIC</h1>\n",
        "<center><h1>2021</h1>\n",
        "<strong>Ciclo 1 </strong><br />\n",
        "PROFESORA ELIZABETH LEON GUZMAN <br />\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xPmCD3Ve2wOz"
      },
      "source": [
        "El objetivo de esta practica es programar en python algunos componentes para realizar análisis de datos textuales usando librerías como nltk, pandas, re, matplotlib y sclearn."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EJXMadImm6Ix"
      },
      "source": [
        "##  **Corpus (conjunto de Datos)**\n",
        "Construiremos un text corpus sencillo sobre el cual demostraremos las diferentes metodologías y modelos de extracción de características."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hpwJZBbFnDBF"
      },
      "source": [
        "# Importamos librerías\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tkNDQ-EMm7Qf"
      },
      "source": [
        "# Creamos un corpus\n",
        "corpus = ['El cielo esta azul y hermoso.',\n",
        "'Me gusta el día de hoy, ¡soleado y cielo azul y hermoso!',\n",
        "'El zorro marrón rápido salta sobre le perro perezoso.',\n",
        "'El desayuno del rey tiene frutas. Tncluye fresas, uvas, y jugo de naranja',\n",
        "'¡Me gusta la ensalada de frutas en epecial que tenga mango, fresas y uvas!',\n",
        "'¡El zorro marrón es rápido y el perro azul es perezoso!',\n",
        "'El cielo esta muy azul y el cielo es muy hermoso hoy',\n",
        "'¡El perro es perezoso pero el zorro marrón es rápido!'\n",
        "]"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eb67aMConbQl"
      },
      "source": [
        "# Asignamos un label para cada frase (documento) \n",
        "labels = ['clima', 'clima', 'animales', 'comida', 'comida', 'animales', 'clima', 'animales']"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OqrMy9elni3h"
      },
      "source": [
        "# Convertimos el corpus a un Pandas dataframe\n",
        "corpus = np.array(corpus)\n",
        "corpus_df = pd.DataFrame({\"Documento\": corpus, \"Categoria\": labels})\n",
        "corpus_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S3mTaSuun29Y"
      },
      "source": [
        "##  **Preprocesamiento (Noramlización del Texto)**\n",
        "\n",
        "Antes de extraer características debemos preprocesar el texto, eliminado los caracteres innecesarios, los signos de puntuación, los stop words, tokenizar, etc.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OX5nA0BbovMI"
      },
      "source": [
        "# Popular corpus y clases\n",
        "nltk.download(\"popular\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0frE-syIn1uo"
      },
      "source": [
        "#Creación de una función para realizar las tareas de normalización\n",
        "wpt = nltk.WordPunctTokenizer()\n",
        "stop_words = nltk.corpus.stopwords.words('spanish')\n",
        "def normalize_document(doc):\n",
        "  # deaj en minúscula y remueve caracteres especiales y espacios en blanco\n",
        "  #doc = re.sub(r'[^a-zA-Z\\s]', '', doc, re.I|re.A)\n",
        "  doc = re.sub(r'[^a-zA-Záéíóú\\s]', '', doc, re.I|re.A)\n",
        "  doc = doc.lower()\n",
        "  doc = doc.strip()\n",
        "  # tokenize document\n",
        "  tokens = wpt.tokenize(doc)\n",
        "  # filter stopwords out of document\n",
        "  filtered_tokens = [token for token in tokens if token not in stop_words]\n",
        "  # re-create document from filtered tokens\n",
        "  doc = ' '.join(filtered_tokens)\n",
        "  return doc"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zVuqvFWnpHxb"
      },
      "source": [
        "# Vectorizamos la función de extracción de características\n",
        "normalize_corpus = np.vectorize(normalize_document)"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BVeW-cuwpIgT"
      },
      "source": [
        "# Normalizamos el corpus\n",
        "norm_corpus = normalize_corpus(corpus)\n",
        "norm_corpus"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EXzVHvWu3Qm-"
      },
      "source": [
        "## **2. Representación del texto**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L4403XUP2FWy"
      },
      "source": [
        "# Unir todas las palabras\n",
        "all_words = \" \".join(norm_corpus)"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KM4RPq_b2Kmy"
      },
      "source": [
        "# Calculamos la frecuencia de las palabras\n",
        "from collections import Counter\n",
        "c = Counter(all_words.split())\n",
        "#c(50)\n",
        "print(c.elements)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nWBCOsorCzvC"
      },
      "source": [
        "#Ordenamos. Podemos seleccionar el número de términos más frecuentes\n",
        "#sort_words = dict(c.most_common())\n",
        "sort_words = dict(c.most_common(300))"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vojAI8RL26US"
      },
      "source": [
        "2.1 Gráfica de Frecuencia de palabras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iJ9rZCSO8gyL"
      },
      "source": [
        "#Extraemos las etiquetas y los valores por separado\n",
        "x_val = sort_words.values()\n",
        "x_label = sort_words.keys()"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "meX4lwD_2LZv"
      },
      "source": [
        "#Dibuja la Frecuencia de la palabras de mayor a menor (distribución de las frecuencias)\n",
        "#plt.figure(figsize=(20,5))\n",
        "plt.figure(figsize=(30,3))\n",
        "plt.bar(x_label, x_val)\n",
        "plt.xlabel(\"Palabra\"); plt.ylabel(\"Frecuencia\")\n",
        "plt.xticks(rotation=90)\n",
        "#plt.title(\"Análisis de frecuencias de palabras en ¿Cuál es tu visión a futuro?\")\n",
        "#plt.savefig('vision.png')\n",
        "#plt.title(\"Análisis de frecuencias de palabras en ¿Por qué desea hacer parte del programa?\")\n",
        "#plt.savefig('programa.png')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VPmdibar24b3"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n",
        "### 2.2. Nube de palabras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R9uBOXw72LNZ"
      },
      "source": [
        "from wordcloud import WordCloud\n",
        "\n",
        "plt.figure(figsize=(10,10))\n",
        "# Creamos la nube de palabras\n",
        "im = WordCloud().generate(all_words)\n",
        "# Visualizamos la nube de palabras\n",
        "plt.imshow(im, interpolation=\"bilinear\")\n",
        "plt.axis(\"off\")\n",
        "#plt.savefig('nubeVision.png')\n",
        "#plt.savefig('nubePrograma.png')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NArYfJMfG5mf"
      },
      "source": [
        "###Leyendo de Archivo\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qqETinEWHKBK"
      },
      "source": [
        "import pandas as pd\n",
        "comentarios = pd.read_excel(\"mintic_2.xlsx\")\n",
        "corpus1 = comentarios['¿Cuál es tu visión a futuro?']\n",
        "print(corpus1.head(5))\n",
        "corpus2 = comentarios['¿Por qué desea hacer parte del programa?']\n",
        "print(corpus2.head(5))\n",
        "#corpus = corpus1 + corpus2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_-5ZZt70RJ3R"
      },
      "source": [
        "import nltk\n",
        "stop_words=nltk.corpus.stopwords.words(\"spanish\")"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iPvR9IeQRg1X"
      },
      "source": [
        "# Normalizamos el corpus\n",
        "norm_corpus = normalize_corpus(corpus1)\n",
        "norm_corpus"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qNBrG2cn29wK"
      },
      "source": [
        "##**Clasificación de documentos**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JbILiaPV3Mzn"
      },
      "source": [
        "Construcción de un modelo de clasificación usando \"NaiveBayes\" para predecir el tema de los documentos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cHvdWmK7vXq"
      },
      "source": [
        "### Bolsa de Palabras - Bag of Words (Bow)\n",
        "\n",
        "Es el modelo de representación vectorial más simple, representa cada documento en el corpus como un vector numérico donde cada dimensión es una palabra especifica en el corpus y el valor es puede ser:\n",
        "*i)* un conteo de frecuencia de la palabra en el documento, \n",
        "*ii)* la ocurrencia o no de la palabra en el documento (0 si la palabra no se encuentra en el documento; 1 en otro caso), \n",
        "*iii)* incluso pueden ser valores de pesados. \n",
        "\n",
        "Este modelo no toma en cuenta la secuencia de palabras, gramática o semántica."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O-be4BUQ46AL"
      },
      "source": [
        "# Importamos el extractor de características BoW de Scikit-learn\n",
        "from sklearn.feature_extraction.text import CountVectorizer"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cF5jD1r95ENi"
      },
      "source": [
        "# Instanciamos el extractor de características para la ocurrencia de palabras\n",
        "cv = CountVectorizer(min_df=0., max_df=1.)\n",
        "# Extraemos las características del corpus \n",
        "cv_matrix = cv.fit_transform(norm_corpus)\n",
        "# El resultado es una matriz sparse\n",
        "cv_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xu4ZjomJ5Jik"
      },
      "source": [
        "El resultado es una matriz dispersa porque el número de palabras puede incrementarse de manera exponencial con cada nuevo documento, pues cada palabra distinta se convierte en una nueva característica. El resultado previo muestra los pares (x,y) de la matriz de características que son distintos de cero, donde x representa a un documento y y representa un termino en el corpus.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n7M24FDt5KeM"
      },
      "source": [
        "# Convertimos de la representación sparse a la densa para visualizarla como numpy array\n",
        "cv_matrix = cv_matrix.toarray()\n",
        "cv_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d7iyr8Ew5P-c"
      },
      "source": [
        "# Obtenemos todas las palabras diferentes en el corpus\n",
        "vocab = cv.get_feature_names()\n",
        "# Mostramos el documento y las features\n",
        "pd.DataFrame(cv_matrix, columns=vocab)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dJzbaNnD88ak"
      },
      "source": [
        "tf-idf **term frequency-inverse document frequency**, es la combinación de  la frecuencia del término (tf) y la inversa de la frecuencia del documento (idf). \n",
        "\n",
        "$$tfidf=tf \\times idf$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JbFZCI655UgB"
      },
      "source": [
        "# Importamos el extractor de características tf-idf de scikit-learn \n",
        "from sklearn.feature_extraction.text import TfidfTransformer"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vqURjqHY5X-5"
      },
      "source": [
        "# Creamos el objeto tf-idf transformer y extraemos las características del corpus \n",
        "tt = TfidfTransformer(norm=\"l2\", use_idf=True)\n",
        "# Obtenemos la matrix tf-idf del modelo BoW \n",
        "tt_matrix = tt.fit_transform(cv_matrix)"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "btOaV28O5adP"
      },
      "source": [
        "# Visualizamos la matrix tf-idf\n",
        "tt_matrix = tt_matrix.toarray()\n",
        "vocab = cv.get_feature_names()\n",
        "data = pd.DataFrame(np.round(tt_matrix, 2), columns=vocab)\n",
        "data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_xdKLUQr3IoO"
      },
      "source": [
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(data, labels, test_size=0.2, random_state=0)\n",
        "print(X_train)\n",
        "print(X_test)\n",
        "print(Y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gx0Y8FQp8MNG"
      },
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "GNB = GaussianNB()\n",
        "GNB.fit(X_train, Y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RkTOHmdC8R_G"
      },
      "source": [
        "Y_pred = GNB.predict(X_test)\n",
        "print(Y_pred)\n",
        "accuracy_score(Y_test,Y_pred)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}